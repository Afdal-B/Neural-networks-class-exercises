{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "XOR = [ (array([0,0]), 0),\n",
    "        (array([0,1]), 1),\n",
    "        (array([1,0]), 1),\n",
    "        (array([1,1]), 0), ]\n",
    "\n",
    "X_train = array([x[0] for x in XOR])\n",
    "y_train = array([x[1] for x in XOR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Sequential()\n",
    "network.add(Input((2,)))\n",
    "network.add(Dense(2, activation='relu'))\n",
    "network.add(Dense(1, activation='sigmoid'))\n",
    "network.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m6\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m3\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> (36.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9\u001b[0m (36.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> (36.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9\u001b[0m (36.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.7500 - loss: 0.7669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13b97fda0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(x=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((network.predict(array([[0, 1]])) > 0.5)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((network.predict(array([[0, 0]])) > 0.5)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ELABORATION DE NOTRE BASE D'APPRENTISSAGE DES LANGUES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>langue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿The Project Gutenberg eBook of The Complete W...</td>\n",
       "      <td>anglais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You may copy it, give it away or re-use it un...</td>\n",
       "      <td>anglais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you are not located in the United States,y...</td>\n",
       "      <td>anglais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Title: The Complete Works of William Shakespea...</td>\n",
       "      <td>anglais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2When forty winters shall ...</td>\n",
       "      <td>anglais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51554</th>\n",
       "      <td>Javert aperçut la soeur et s'arrêta interdit</td>\n",
       "      <td>français</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51555</th>\n",
       "      <td>On se rappelle que le fond même de Javert, son...</td>\n",
       "      <td>français</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51556</th>\n",
       "      <td>Il était tout d'unepièce et n'admettait ni ob...</td>\n",
       "      <td>français</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51557</th>\n",
       "      <td>Pour lui, bienentendu, l'autorité ecclésiasti...</td>\n",
       "      <td>français</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51558</th>\n",
       "      <td>Il étaitreligieux, superficiel et correct sur...</td>\n",
       "      <td>français</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51559 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   texte    langue\n",
       "0      ﻿The Project Gutenberg eBook of The Complete W...   anglais\n",
       "1       You may copy it, give it away or re-use it un...   anglais\n",
       "2       If you are not located in the United States,y...   anglais\n",
       "3      Title: The Complete Works of William Shakespea...   anglais\n",
       "4                          2When forty winters shall ...   anglais\n",
       "...                                                  ...       ...\n",
       "51554       Javert aperçut la soeur et s'arrêta interdit  français\n",
       "51555  On se rappelle que le fond même de Javert, son...  français\n",
       "51556   Il était tout d'unepièce et n'admettait ni ob...  français\n",
       "51557   Pour lui, bienentendu, l'autorité ecclésiasti...  français\n",
       "51558   Il étaitreligieux, superficiel et correct sur...  français\n",
       "\n",
       "[51559 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les fichiers de texte anglais et français\n",
    "with open('english.txt', 'r') as file:\n",
    "    english_text = file.read().strip()\n",
    "\n",
    "with open('french.txt', 'r') as file:\n",
    "    french_text = file.read().strip()\n",
    "# Nettoyer le texte en retirant les sauts de ligne\n",
    "english_text_nettoye = english_text.replace('\\n', '')\n",
    "french_text_nettoye = french_text.replace('\\n', '')\n",
    "# Séparer les textes en phrases\n",
    "english_phrases = english_text_nettoye.split('.')\n",
    "french_phrases = french_text_nettoye.split('.')\n",
    "# Nous allons garder que les phrases qui contiennent plus de 5 mots et eliminer les phrases en anglais(Les credits en fin de livre) présents dans les textes\n",
    "english_phrases_filtrees = [phrase for phrase in english_phrases if len(phrase.split()) > 5]\n",
    "french_phrases_filtrees = [phrase for phrase in french_phrases[:-250] if len(phrase.split()) > 5]\n",
    "# Créer un dataframe avec les phrases anglaises et françaises filtrées\n",
    "df = pd.DataFrame({'texte': english_phrases_filtrees + french_phrases_filtrees, 'langue': ['anglais']*len(english_phrases_filtrees) + ['français']*len(french_phrases_filtrees)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **PREPARATION DES DONNÉES À L'ENTRAINEMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TextVectorization(max_tokens=10000, output_mode='tf-idf')\n",
    "vectorizer.adapt(df['texte'])\n",
    "# Vectoriser les données\n",
    "X = vectorizer(df['texte'])\n",
    "# Encoder les labels \n",
    "y = tf.where(df['langue'] == 'français', 1, 0)\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "TAILLE_TEST = int(0.2 * len(X))\n",
    "indices = tf.range(len(X))\n",
    "indices_melanges = tf.random.shuffle(indices)\n",
    "X = tf.gather(X, indices_melanges)\n",
    "y = tf.gather(y, indices_melanges)\n",
    "X_train = X[TAILLE_TEST:]\n",
    "X_test = X[:TAILLE_TEST]\n",
    "y_train = y[TAILLE_TEST:]\n",
    "y_test = y[:TAILLE_TEST]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CREATION DU MODÈLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Afdal/Documents/GitHub/Neural-networks-class-exercises/Neural-networks-class-exercises/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4125/4125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - accuracy: 0.9869 - loss: 0.0599\n",
      "Epoch 2/10\n",
      "\u001b[1m4125/4125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - accuracy: 0.9994 - loss: 0.0028\n",
      "Epoch 3/10\n",
      "\u001b[1m4125/4125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - accuracy: 0.9997 - loss: 0.0011\n",
      "Epoch 4/10\n",
      "\u001b[1m4125/4125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 5.3881e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m4125/4125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 4.4550e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m4125/4125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 4.2405e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m4125/4125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 3.6835e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m4125/4125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 4.3849e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m4125/4125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 3.2930e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m4125/4125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - accuracy: 0.9999 - loss: 3.3230e-04\n",
      "\u001b[1m323/323\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9997 - loss: 0.0044\n",
      "Précision: 99.95\n"
     ]
    }
   ],
   "source": [
    "# le model \n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Entrainement \n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10)\n",
    "# Évaluation du modèle\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Précision: %.2f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction: 1\n"
     ]
    }
   ],
   "source": [
    "# Tester le modèle avec une nouvelle phrase\n",
    "phrase = \"Je me nomme Afdal\"\n",
    "inputs = vectorizer([phrase])\n",
    "prediction = model(inputs, training=False)\n",
    "prediction_binaire = tf.where(prediction >= 0.5, 1, 0)\n",
    "print(\"Prédiction:\", prediction_binaire.numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction: 0\n"
     ]
    }
   ],
   "source": [
    "phrase = \"My name is Afdal\"\n",
    "inputs = vectorizer([phrase])\n",
    "prediction = model(inputs, training=False)\n",
    "prediction_binaire = tf.where(prediction >= 0.5, 1, 0)\n",
    "print(\"Prédiction:\", prediction_binaire.numpy()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DANS LE CAS DE PLUSIEURS LANGUES (4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>langue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿The Project Gutenberg eBook of The Complete W...</td>\n",
       "      <td>anglais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You may copy it, give it away or re-use it un...</td>\n",
       "      <td>anglais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you are not located in the United States,y...</td>\n",
       "      <td>anglais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Title: The Complete Works of William Shakespea...</td>\n",
       "      <td>anglais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2When forty winters shall ...</td>\n",
       "      <td>anglais</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52932</th>\n",
       "      <td>Hän peitti kettu-naisenkuin pienen lapsen huo...</td>\n",
       "      <td>finnois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52933</th>\n",
       "      <td>Yöstä huolimatta jatkoi matkue kulkuansa Tokioon</td>\n",
       "      <td>finnois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52934</th>\n",
       "      <td>Iltaruoka oliviety koskemattomana takaisin; p...</td>\n",
       "      <td>finnois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52935</th>\n",
       "      <td>Hän katsoa tuijotti jotakin kädessäänolevaa e...</td>\n",
       "      <td>finnois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52936</th>\n",
       "      <td>Se ei ollut kättä isompi, kulunut, repaleinen...</td>\n",
       "      <td>finnois</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52937 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   texte   langue\n",
       "0      ﻿The Project Gutenberg eBook of The Complete W...  anglais\n",
       "1       You may copy it, give it away or re-use it un...  anglais\n",
       "2       If you are not located in the United States,y...  anglais\n",
       "3      Title: The Complete Works of William Shakespea...  anglais\n",
       "4                          2When forty winters shall ...  anglais\n",
       "...                                                  ...      ...\n",
       "52932   Hän peitti kettu-naisenkuin pienen lapsen huo...  finnois\n",
       "52933   Yöstä huolimatta jatkoi matkue kulkuansa Tokioon  finnois\n",
       "52934   Iltaruoka oliviety koskemattomana takaisin; p...  finnois\n",
       "52935   Hän katsoa tuijotti jotakin kädessäänolevaa e...  finnois\n",
       "52936   Se ei ollut kättä isompi, kulunut, repaleinen...  finnois\n",
       "\n",
       "[52937 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les fichiers de texte pour chaque langue\n",
    "with open('english.txt', 'r') as file:\n",
    "    english_text = file.read().strip()\n",
    "\n",
    "with open('french.txt', 'r') as file:\n",
    "    french_text = file.read().strip()\n",
    "\n",
    "with open('espagnol.txt', 'r') as file:\n",
    "    espagnol_text = file.read().strip()\n",
    "\n",
    "with open('finnish.txt', 'r') as file:\n",
    "    finnish_text = file.read().strip()\n",
    "\n",
    "# Nettoyer le texte en retirant les sauts de ligne \\n\n",
    "english_text_nettoye = english_text.replace('\\n', '')\n",
    "french_text_nettoye = french_text.replace('\\n', '')\n",
    "espagnol_text_nettoye = espagnol_text.replace('\\n', '')\n",
    "finnish_text_nettoye = finnish_text.replace('\\n', '')\n",
    "\n",
    "#  On sépare les textes en phrase\n",
    "english_phrases = english_text_nettoye.split('.')\n",
    "french_phrases = french_text_nettoye.split('.')\n",
    "espagnol_phrases = espagnol_text_nettoye.split('.')\n",
    "finnish_phrases = finnish_text_nettoye.split('.')\n",
    "\n",
    "# Nous allons garder que les phrases qui contiennent plus de 5 mots et eliminer les phrases en anglais(Les credits en fin de livre) présents dans les textes\n",
    "english_phrases_filtrees = [phrase for phrase in english_phrases if len(phrase.split()) > 5]\n",
    "french_phrases_filtrees = [phrase for phrase in french_phrases[:-250] if len(phrase.split()) > 5]\n",
    "espagnol_phrases_filtrees = [phrase for phrase in espagnol_phrases[:-250] if len(phrase.split()) > 5]\n",
    "finnish_phrases_filtrees = [phrase for phrase in finnish_phrases[:-250] if len(phrase.split()) > 5]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'texte': english_phrases_filtrees + french_phrases_filtrees + espagnol_phrases_filtrees + finnish_phrases_filtrees,\n",
    "    'langue': ['anglais']*len(english_phrases_filtrees) + ['français']*len(french_phrases_filtrees) + ['espagnol']*len(espagnol_phrases_filtrees) + ['finnois']*len(finnish_phrases_filtrees)\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **PREPARATION DES DONNÉES À L'ENTRAINEMENT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TextVectorization(max_tokens=10000, output_mode='tf-idf')\n",
    "vectorizer.adapt(df['texte'])\n",
    "\n",
    "# Vectoriser les données\n",
    "X = vectorizer(df['texte'])\n",
    "\n",
    "# Encoder les labels \n",
    "y = tf.where(df['langue'] == 'français', 0, \n",
    "    tf.where(df['langue'] == 'anglais', 1, \n",
    "    tf.where(df['langue'] == 'espagnol', 2, 3)))\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "TAILLE_TEST = int(0.2 * len(X))\n",
    "indices = tf.range(len(X))\n",
    "indices_melanges = tf.random.shuffle(indices)\n",
    "\n",
    "X = tf.gather(X, indices_melanges)\n",
    "y = tf.gather(y, indices_melanges)\n",
    "\n",
    "X_train = X[TAILLE_TEST:]\n",
    "X_test = X[:TAILLE_TEST]\n",
    "y_train = y[TAILLE_TEST:]\n",
    "y_test = y[:TAILLE_TEST]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DEFINITION DU MODÈLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Afdal/Documents/GitHub/Neural-networks-class-exercises/Neural-networks-class-exercises/.venv/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,280,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,644</span> (4.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,280,644\u001b[0m (4.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,644</span> (4.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,280,644\u001b[0m (4.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1324/1324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 21ms/step - accuracy: 0.9824 - loss: 0.0824 - val_accuracy: 0.9983 - val_loss: 0.0100\n",
      "Epoch 2/10\n",
      "\u001b[1m1324/1324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 26ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 0.9982 - val_loss: 0.0070\n",
      "Epoch 3/10\n",
      "\u001b[1m1324/1324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 26ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.9984 - val_loss: 0.0105\n",
      "Epoch 4/10\n",
      "\u001b[1m1324/1324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 24ms/step - accuracy: 0.9997 - loss: 9.3779e-04 - val_accuracy: 0.9982 - val_loss: 0.0114\n",
      "Epoch 5/10\n",
      "\u001b[1m1324/1324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 29ms/step - accuracy: 0.9996 - loss: 0.0019 - val_accuracy: 0.9984 - val_loss: 0.0122\n",
      "Epoch 6/10\n",
      "\u001b[1m1324/1324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 30ms/step - accuracy: 0.9998 - loss: 8.6816e-04 - val_accuracy: 0.9981 - val_loss: 0.0150\n",
      "Epoch 7/10\n",
      "\u001b[1m1324/1324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 26ms/step - accuracy: 0.9998 - loss: 7.8753e-04 - val_accuracy: 0.9984 - val_loss: 0.0160\n",
      "Epoch 8/10\n",
      "\u001b[1m1324/1324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 26ms/step - accuracy: 0.9998 - loss: 0.0016 - val_accuracy: 0.9984 - val_loss: 0.0161\n",
      "Epoch 9/10\n",
      "\u001b[1m1324/1324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 25ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.9985 - val_loss: 0.0148\n",
      "Epoch 10/10\n",
      "\u001b[1m1324/1324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 29ms/step - accuracy: 0.9997 - loss: 9.6028e-04 - val_accuracy: 0.9983 - val_loss: 0.0186\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9988 - loss: 0.0122\n",
      "Test accuracy: 0.9983\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(X.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "Phrase: Rester soi-même dans un monde qui tente constamment de te changer est le plus grand accomplissement.\n",
      "Langue prédite: français\n",
      "\n",
      "Phrase: Retiens ceci mon élève : If you're determined to learn, no one can stop you.\n",
      "Langue prédite: anglais\n",
      "\n",
      "Phrase: Más vale prevenir que curar\n",
      "Langue prédite: espagnol\n",
      "\n",
      "Phrase: Hotellivirkailija: Huoneissa tai parvekkeilla ei saa tupakoida.Teemu: Tupakoiminen on vaarallista terveydelle.Hotellivirkailija: Aivan\n",
      "Langue prédite: finnois\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exemples_phrases = [\n",
    "    \"Rester soi-même dans un monde qui tente constamment de te changer est le plus grand accomplissement.\",\n",
    "    \"Retiens ceci mon élève : If you're determined to learn, no one can stop you.\",\n",
    "    \"Más vale prevenir que curar\",\n",
    "    \"Hotellivirkailija: Huoneissa tai parvekkeilla ei saa tupakoida.Teemu: Tupakoiminen on vaarallista terveydelle.Hotellivirkailija: Aivan\"\n",
    "]\n",
    "\n",
    "X_exemples = vectorizer(exemples_phrases)\n",
    "\n",
    "# Nous allons prédire les langues pour les exemples de texte\n",
    "y_pred_exemples = model.predict(X_exemples)\n",
    "y_pred_exemples_classes = tf.argmax(y_pred_exemples, axis=1)\n",
    "\n",
    "\n",
    "for i, phrase in enumerate(exemples_phrases):\n",
    "    print(f\"Phrase: {phrase}\")\n",
    "    print(f\"Langue prédite: {['français', 'anglais', 'espagnol', 'finnois'][y_pred_exemples_classes[i]]}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
